import { GoogleGenerativeAI } from '@google/generative-ai';

function required(name: string): string {
  const v = process.env[name];
  if (!v) throw new Error(`Missing env ${name}`);
  return v;
}

export type GeminiResult = {
  reply: string;
  next_stage?: string;
  lead_type?: 'unknown' | 'candidate' | 'agency';
  need_link?: boolean;
  stop?: boolean;
  tags?: Record<string, string>;
  memory_update?: string;
};

const SYSTEM_FALLBACK: GeminiResult = {
  reply: '–ü–æ–Ω—è–ª. –ù–∞–ø–∏—à–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: —Å—Ç—Ä–∞–Ω–∞ –∏ –∫–∞–∫–∞—è —Ä–∞–±–æ—Ç–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: –ì–µ—Ä–º–∞–Ω–∏—è, —Å–∫–ª–∞–¥).',
  next_stage: 'ask_country_job',
  lead_type: 'unknown',
};

const DEFAULT_MODELS = ['gemini-2.5-flash', 'gemini-2.0-flash'];

// Cache the last successful model to avoid retrying failed ones
let cachedWorkingModel: string | null = null;

export async function callGemini(args: {
  systemPrompt: string;
  userText: string;
  memory: { summary: string; recent: { direction: 'in' | 'out'; text: string }[] };
  stage: string;
}): Promise<GeminiResult> {
  const apiKey = process.env.GEMINI_API_KEY || '';
  if (!apiKey) throw new Error('GEMINI_API_KEY is missing');

  const envModel = process.env.GEMINI_MODEL;
  let models = envModel
    ? [envModel, ...DEFAULT_MODELS.filter(m => m !== envModel)]
    : DEFAULT_MODELS;

  // If we have a cached working model, try it first
  if (cachedWorkingModel && !envModel) {
    models = [cachedWorkingModel, ...models.filter(m => m !== cachedWorkingModel)];
  }

  const genAI = new GoogleGenerativeAI(apiKey);

  const recentLines = args.memory.recent
    .slice(-12)
    .map(m => (m.direction === 'in' ? `USER: ${m.text}` : `BOT: ${m.text}`))
    .join('\n');

  const prompt = [
    '–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON.',
    '–í–ê–ñ–ù–û: –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¢–û–õ–¨–ö–û JSON –æ–±—ä–µ–∫—Ç–æ–º, –±–µ–∑ markdown –±–ª–æ–∫–æ–≤ (–±–µ–∑ ```json ... ```).',
    '–°—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON: { "reply": "—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞", "next_stage": "–Ω–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞", "lead_type": "unknown/candidate/agency", "need_link": false, "stop": false, "memory_update": "–Ω–æ–≤–∞—è –∏–Ω—Ñ–æ" }',
    '–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ MEMORY_SUMMARY.',
    '–ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ - –ø–µ—Ä–µ—Å–ø—Ä–æ—Å–∏, –Ω–æ –≤ –ø–æ–ª–µ reply.',
    '',
    'SYSTEM_PROMPT:',
    args.systemPrompt,
    '',
    'CURRENT_STAGE:',
    args.stage,
    '',
    'MEMORY_SUMMARY (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ):',
    args.memory.summary || '(–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)',
    '',
    'RECENT_DIALOG (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è):',
    recentLines || '(–¥–∏–∞–ª–æ–≥ –ø—É—Å—Ç)',
    '',
    'USER_MESSAGE (–Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ):',
    args.userText,
  ].join('\n');

  let lastError = null;

  for (const modelName of models) {
    try {
      console.log(`ü§ñ [Gemini] Trying model: ${modelName}...`);
      const model = genAI.getGenerativeModel({ model: modelName });
      const result = await model.generateContent(prompt);
      const raw = result.response.text().trim();

      const jsonText = extractJson(raw);
      const parsed = JSON.parse(jsonText);

      if (!parsed || typeof parsed.reply !== 'string' || !parsed.reply.trim()) {
        console.warn(`‚ö†Ô∏è [Gemini] Model ${modelName} returned invalid JSON, skipping...`);
        continue; // Try next model if response is bad
      }

      // Cache this model as working
      if (cachedWorkingModel !== modelName) {
        cachedWorkingModel = modelName;
        console.log(`‚úÖ [Gemini] Cached working model: ${modelName}`);
      }

      const reply = String(parsed.reply).slice(0, 500);
      return {
        reply,
        next_stage: typeof parsed.next_stage === 'string' ? parsed.next_stage : undefined,
        lead_type: ['unknown', 'candidate', 'agency'].includes(parsed.lead_type) ? parsed.lead_type : 'unknown',
        need_link: typeof parsed.need_link === 'boolean' ? parsed.need_link : undefined,
        stop: typeof parsed.stop === 'boolean' ? parsed.stop : undefined,
        memory_update: typeof parsed.memory_update === 'string' ? parsed.memory_update.slice(0, 2000) : undefined,
      };

    } catch (e: any) {
      console.error(`‚ùå [Gemini] Model ${modelName} failed:`, e.message);
      lastError = e;
      // Continue to next model
    }
  }

  console.error('‚ùåCRITICAL [Gemini] All models failed.');
  cachedWorkingModel = null; // Reset cache if all failed
  return SYSTEM_FALLBACK;
}

function extractJson(s: string): string {
  const first = s.indexOf('{');
  const last = s.lastIndexOf('}');
  if (first >= 0 && last > first) return s.slice(first, last + 1);
  return s;
}
